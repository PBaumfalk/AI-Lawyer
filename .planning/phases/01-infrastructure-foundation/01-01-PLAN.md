---
phase: 01-infrastructure-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - Dockerfile
  - .env.example
  - prisma/schema.prisma
  - src/lib/redis.ts
  - src/lib/logger.ts
  - src/lib/queue/connection.ts
  - src/lib/queue/queues.ts
  - src/lib/queue/processors/test.processor.ts
  - src/lib/socket/emitter.ts
  - src/worker.ts
  - src/app/api/health/route.ts
  - src/app/api/jobs/[jobId]/retry/route.ts
  - scripts/build-server.ts
  - scripts/build-worker.ts
  - package.json
  - docker-entrypoint.sh
autonomous: true
# NOTE: 17 files exceeds 15-file threshold but all are tightly coupled infrastructure
# (Redis, queue, logger, worker, Docker, Prisma). Splitting would create artificial seams.
requirements:
  - REQ-IF-001
  - REQ-IF-002
  - REQ-IF-005

must_haves:
  truths:
    - "Redis 7 container is running in Docker Compose and both app and worker connect to it"
    - "A test BullMQ job enqueued from a Next.js API route is picked up and processed by the worker within 5 seconds"
    - "Worker process shuts down gracefully (in-flight jobs complete, no data loss) when receiving SIGTERM"
    - "Health check endpoint returns JSON with status of Redis, PostgreSQL, and overall system health"
    - "Structured JSON logs are produced by both app and worker processes"
  artifacts:
    - path: "docker-compose.yml"
      provides: "Redis 7 service + worker service definitions"
      contains: "redis"
    - path: "src/lib/redis.ts"
      provides: "Shared Redis connection factory"
      exports: ["createRedisConnection"]
    - path: "src/lib/logger.ts"
      provides: "Pino-based structured logger with context support"
      exports: ["rootLogger", "createLogger"]
    - path: "src/lib/queue/queues.ts"
      provides: "BullMQ queue definitions used by both app and worker"
      exports: ["testQueue"]
    - path: "src/worker.ts"
      provides: "Worker process entrypoint with BullMQ processors and graceful shutdown"
      contains: "gracefulShutdown"
    - path: "src/app/api/health/route.ts"
      provides: "Health check endpoint"
      exports: ["GET"]
    - path: "Dockerfile"
      provides: "Multi-stage build with esbuild bundling for server.ts and worker.ts"
      contains: "dist-worker"
    - path: "src/app/api/jobs/[jobId]/retry/route.ts"
      provides: "Job retry endpoint for failed job re-enqueue"
      exports: ["POST"]
    - path: "prisma/schema.prisma"
      provides: "Notification and SystemSetting models"
      contains: "model Notification"
  key_links:
    - from: "src/worker.ts"
      to: "src/lib/queue/queues.ts"
      via: "imports queue connection config"
      pattern: "import.*queue"
    - from: "src/worker.ts"
      to: "src/lib/redis.ts"
      via: "creates Redis connection with maxRetriesPerRequest: null"
      pattern: "createRedisConnection"
    - from: "src/lib/queue/queues.ts"
      to: "src/lib/redis.ts"
      via: "uses shared Redis connection factory"
      pattern: "createRedisConnection"
    - from: "docker-compose.yml"
      to: "Dockerfile"
      via: "worker service uses same image with different CMD"
      pattern: "dist-worker"
---

<objective>
Set up the core infrastructure foundation: Redis 7 in Docker Compose, BullMQ job queue with typed queues, a separate worker process with graceful shutdown, esbuild bundling for server.ts and worker.ts, structured logging via Pino, health check endpoint, and Prisma schema additions for Notification and SystemSetting models.

Purpose: This is the foundation that every subsequent phase depends on. Background job processing (email sync, OCR, AI tasks, PDF generation) all require a worker process with Redis-backed queues. The health check provides operational monitoring. The Prisma models enable the notification and settings systems built in later plans.

Output: Working Redis + worker Docker setup, shared libraries (redis, logger, queues), health endpoint, Prisma schema updates, esbuild build pipeline.
</objective>

<execution_context>
@/Users/patrickbaumfalk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrickbaumfalk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-infrastructure-foundation/01-RESEARCH.md
@.planning/phases/01-infrastructure-foundation/01-CONTEXT.md
@docker-compose.yml
@Dockerfile
@next.config.mjs
@prisma/schema.prisma
@src/lib/db.ts
@src/lib/auth.ts
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install dependencies, create shared libs (Redis, Logger, Queue), add Prisma models</name>
  <files>
    package.json
    src/lib/redis.ts
    src/lib/logger.ts
    src/lib/queue/connection.ts
    src/lib/queue/queues.ts
    src/lib/queue/processors/test.processor.ts
    src/lib/socket/emitter.ts
    prisma/schema.prisma
    .env.example
  </files>
  <action>
    1. Install all Phase 1 dependencies:
       ```
       npm install bullmq ioredis socket.io socket.io-client @socket.io/redis-adapter @socket.io/redis-emitter pino pino-roll @bull-board/api @bull-board/hono hono
       npm install -D pino-pretty esbuild @types/node
       ```

    2. Create `src/lib/redis.ts` — Shared Redis connection factory:
       - Export `createRedisConnection(options?: { maxRetriesPerRequest?: number | null }): Redis`
       - Default maxRetriesPerRequest: 20 (fail-fast for API routes)
       - Workers MUST pass `maxRetriesPerRequest: null` (per BullMQ requirement)
       - Exponential retry strategy: `Math.max(Math.min(Math.exp(times), 20000), 1000)`
       - Log connection events (error, connect) using the logger
       - Read from `process.env.REDIS_URL` with fallback `redis://localhost:6379`
       - DO NOT use ioredis `keyPrefix` option (incompatible with BullMQ)

    3. Create `src/lib/logger.ts` — Pino structured logger:
       - Export `rootLogger` (base pino instance) and `createLogger(module: string)` (child logger)
       - Per user decision: structured JSON logs with fields: timestamp (ISO), level, message, context (userId, akteId, jobId)
       - Dev mode (`NODE_ENV !== 'production'`): use `pino-pretty` transport with colorize
       - Production: multi-transport — stdout (pino/file destination 1) + file rotation via `pino-roll`:
         - File path from `LOG_FILE_PATH` env var, default `/var/log/ai-lawyer/app`
         - Frequency: daily, size: 10m, mkdir: true, extension: .log
       - Base fields: `{ service: "ai-lawyer" }`
       - Timestamp: `pino.stdTimeFunctions.isoTime`
       - Log level from `LOG_LEVEL` env var, default `info`

    4. Create `src/lib/queue/connection.ts` — BullMQ connection config:
       - Export `getQueueConnection()` for use by Queue instances (default maxRetriesPerRequest)
       - Export `getWorkerConnection()` for use by Worker instances (maxRetriesPerRequest: null)
       - Both use `createRedisConnection` from redis.ts

    5. Create `src/lib/queue/queues.ts` — Queue definitions:
       - Export `testQueue` (Queue instance named "test") using getQueueConnection()
       - Default job options per user decision:
         - attempts: 3
         - backoff: custom type
         - removeOnComplete: { age: 86400 } (24h)
         - removeOnFail: { age: 604800 } (7 days)
       - Export `calculateBackoff(attemptsMade: number): number` — custom backoff: [10000, 60000, 300000] (10s, 60s, 5min per user decision)
       - Export `ALL_QUEUES` array for Bull Board auto-discovery

    6. Create `src/lib/queue/processors/test.processor.ts`:
       - Export a processor function that logs job data, waits 1 second (simulate work), and returns `{ success: true }`
       - Use createLogger("test-processor") for logging
       - Accept job data type: `{ userId: string; message: string }`

    7. Create `src/lib/socket/emitter.ts` — Redis emitter singleton for worker-to-browser notifications:
       - Export `getSocketEmitter(): Emitter`
       - Lazy-initialize: create ioredis client + Emitter on first call, cache as singleton
       - Used by worker processors to emit events to user/akte/role rooms

    8. Add to `prisma/schema.prisma` — Two new models at the end of the file:
       ```prisma
       model Notification {
         id          String   @id @default(cuid())
         userId      String
         user        User     @relation(fields: [userId], references: [id], onDelete: Cascade)
         type        String   // e.g. "job:completed", "job:failed", "document:created"
         title       String
         message     String
         data        Json?    // Arbitrary payload (link target, jobId, akteId, etc.)
         read        Boolean  @default(false)
         dismissed   Boolean  @default(false)
         soundType   String?  // Optional sound identifier
         createdAt   DateTime @default(now())

         @@index([userId, read])
         @@index([userId, createdAt])
         @@map("notifications")
       }

       model SystemSetting {
         id        String   @id @default(cuid())
         key       String   @unique
         value     String   @db.Text
         type      String   @default("string") // string, number, boolean, json
         category  String   @default("general")
         label     String?  // German display label
         updatedAt DateTime @updatedAt
         createdAt DateTime @default(now())

         @@index([category])
         @@map("system_settings")
       }
       ```
       Also add `notifications Notification[]` relation field to the existing User model.

    9. Update `.env.example` with new env vars (documented with description, default, required/optional):
       - REDIS_URL (required, default redis://localhost:6379)
       - LOG_LEVEL (optional, default info, options: trace/debug/info/warn/error/fatal)
       - LOG_FILE_PATH (optional, default /var/log/ai-lawyer/app)
       - WORKER_CONCURRENCY (optional, default 5, description: Number of concurrent jobs per queue)
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer && npx prisma validate && npx tsc --noEmit --pretty 2>&1 | head -30</automated>
    <manual>Check that all new files exist and have correct exports</manual>
  </verify>
  <done>
    - redis.ts exports createRedisConnection with configurable maxRetriesPerRequest
    - logger.ts exports rootLogger and createLogger with dev/prod transport switching
    - Queue definitions export testQueue with correct backoff config (10s/60s/5min)
    - test.processor.ts exports a typed processor function
    - emitter.ts exports getSocketEmitter singleton
    - Prisma schema has Notification and SystemSetting models with correct indexes
    - .env.example documents all new env vars
    - All dependencies installed, TypeScript compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create worker.ts, esbuild build scripts, update Dockerfile and Docker Compose</name>
  <files>
    src/worker.ts
    scripts/build-server.ts
    scripts/build-worker.ts
    Dockerfile
    docker-compose.yml
    docker-entrypoint.sh
    package.json
  </files>
  <action>
    1. Create `scripts/build-server.ts` — esbuild bundle for server.ts:
       - Use esbuild to bundle `src/server.ts` to `dist-server/index.js`
       - Settings: bundle: true, platform: node, target: node18, format: esm
       - External: next, sharp, @prisma/client (these come from node_modules at runtime)
       - Banner: `import { createRequire } from "module"; const require = createRequire(import.meta.url);`
       - Handle `@/*` path alias: use esbuild-plugin-alias or configure alias option to map `@/` to `./src/`
         Note: esbuild has a built-in `alias` option (esbuild >= 0.18): `alias: { '@': './src' }` — try this first.
         If path alias resolution fails, install `esbuild-plugin-tsc` as fallback.

    2. Create `scripts/build-worker.ts` — esbuild bundle for worker.ts:
       - Bundle `src/worker.ts` to `dist-worker/index.js`
       - Same settings as server bundle EXCEPT:
         - External: @prisma/client only (worker does NOT need next or sharp)
         - Same ESM banner for createRequire
       - Same path alias handling as build-server.ts

    3. Create `src/worker.ts` — Worker process entrypoint:
       - Import and create a Redis connection via `createRedisConnection({ maxRetriesPerRequest: null })`
       - Import `getSocketEmitter` from socket/emitter for sending notifications to browsers
       - Register BullMQ Worker for "test" queue using the test processor:
         - Concurrency: parseInt(process.env.WORKER_CONCURRENCY || "5")
         - On completion: emit notification to `user:{userId}` room via socketEmitter
         - On failure (after all retries exhausted): emit failure notification to `user:{userId}` AND `role:ADMIN` rooms
           - User message: friendly German "Vorgang fehlgeschlagen. Erneut versuchen?"
           - Admin message: include error details
         - Use calculateBackoff from queues.ts for custom backoff strategy
       - Worker error handler: log errors via createLogger("worker")
       - Implement graceful shutdown:
         - Register SIGINT and SIGTERM handlers
         - On signal: log signal, call `await Promise.all(workers.map(w => w.close()))`, log completion, exit 0
         - worker.close() waits for in-flight jobs to complete (BullMQ built-in behavior)
       - Register uncaughtException and unhandledRejection handlers (log and continue for rejection, log for exception)
       - Subscribe to Redis `settings:changed` channel for runtime settings updates (log level changes)
       - Log "Worker started" on successful initialization

    4. Update `docker-compose.yml`:
       - Add `redis` service:
         - Image: redis:7-alpine
         - Container name: ailawyer-redis
         - restart: unless-stopped
         - Command: `redis-server --appendonly yes --maxmemory-policy noeviction --save 60 1000`
         - Port: 6379:6379
         - Volume: redisdata:/data
         - Healthcheck: redis-cli ping, interval 5s, timeout 3s, retries 5
       - Add `worker` service:
         - build: . (same Dockerfile as app)
         - Container name: ailawyer-worker
         - restart: unless-stopped
         - Command: ["node", "dist-worker/index.js"]
         - stop_grace_period: 30s (per user decision: allow graceful shutdown time)
         - Environment: NODE_ENV, REDIS_URL (redis://redis:6379), DATABASE_URL, LOG_LEVEL, LOG_FILE_PATH (/var/log/ai-lawyer/worker), NEXTAUTH_SECRET (for socket emitter auth context)
         - depends_on: redis (service_healthy), db (service_healthy)
         - Volume: worker_logs:/var/log/ai-lawyer
       - Add REDIS_URL to existing `app` service environment: redis://redis:6379
       - Add `app` depends_on: redis (service_healthy) in addition to existing dependencies
       - Add volumes: redisdata, worker_logs

    5. Update `Dockerfile`:
       - In builder stage, after `RUN npm run build`, add esbuild bundling steps:
         ```
         RUN npx tsx scripts/build-server.ts
         RUN npx tsx scripts/build-worker.ts
         ```
       - In runner stage, copy the bundled outputs:
         ```
         COPY --from=builder /app/dist-server ./dist-server
         COPY --from=builder /app/dist-worker ./dist-worker
         ```
       - Change CMD from default Next.js server to custom server:
         In docker-entrypoint.sh, change the final exec line from `node server.js` to `node dist-server/index.js`
         (Keep the entrypoint script for Prisma migrations + seeding before server start)

    6. Update `docker-entrypoint.sh`:
       - Change the server start command from `exec node server.js` to `exec node dist-server/index.js`
       - Keep all existing logic (db readiness, prisma db push, seeding)

    7. Add npm scripts to `package.json`:
       - "build:server": "tsx scripts/build-server.ts"
       - "build:worker": "tsx scripts/build-worker.ts"
       - "build:custom": "npm run build:server && npm run build:worker"
       - "dev:worker": "tsx --watch src/worker.ts" (for development)
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer && npx tsx scripts/build-worker.ts 2>&1 && ls -la dist-worker/index.js && echo "Worker bundle OK"</automated>
    <manual>Verify dist-worker/index.js exists and is a valid ESM bundle</manual>
  </verify>
  <done>
    - worker.ts exists with BullMQ workers, graceful shutdown handlers, and socket emitter integration
    - build-server.ts and build-worker.ts produce valid ESM bundles in dist-server/ and dist-worker/
    - Docker Compose has redis and worker services with correct health checks, restart policies, and stop_grace_period
    - Dockerfile bundles server.ts and worker.ts during build stage and copies to runner
    - docker-entrypoint.sh starts custom server (dist-server/index.js) instead of default Next.js server
    - npm scripts for build:server, build:worker, build:custom, dev:worker exist
  </done>
</task>

<task type="auto">
  <name>Task 3: Create health check endpoint and test job enqueue API route</name>
  <files>
    src/app/api/health/route.ts
    src/app/api/admin/test-job/route.ts
    src/app/api/jobs/[jobId]/retry/route.ts
  </files>
  <action>
    1. Create `src/app/api/health/route.ts` — Full health check endpoint:
       - Per user decision: returns JSON with status of every service (app, worker, Redis, PostgreSQL, MinIO, Meilisearch, OnlyOffice)
       - Check PostgreSQL: `prisma.$queryRaw\`SELECT 1\`` with latency measurement
       - Check Redis: create temporary ioredis connection with connectTimeout: 3000, maxRetriesPerRequest: 1, ping, then quit
       - Check MinIO: try HEAD request to MinIO endpoint (process.env.MINIO_ENDPOINT + port) or use S3 client listBuckets
       - Check Meilisearch: fetch health endpoint (`${MEILISEARCH_URL}/health`)
       - Check OnlyOffice: fetch healthcheck endpoint (`${ONLYOFFICE_INTERNAL_URL}/healthcheck`)
       - Check Worker: use Redis to check BullMQ worker presence — query `bull:test:id` key or check if any workers are registered via Queue.getWorkers() (BullMQ provides this)
       - Each check wrapped in try-catch with timeout; returns `{ status: "healthy"|"unhealthy", latency?: number }`
       - Aggregate: if all healthy → 200 + `{ status: "healthy", services: {...} }`, otherwise → 503 + `{ status: "degraded", services: {...} }`
       - Include `timestamp` and `uptime` (process.uptime()) in response
       - This endpoint is public (no auth required) for Docker health checks

    2. Create `src/app/api/admin/test-job/route.ts` — Test job enqueue endpoint (for verification):
       - POST handler: requires auth + ADMIN role check
       - Enqueues a test job to the "test" queue with `{ userId: session.user.id, message: "Test job from admin" }`
       - Returns 201 with `{ jobId: job.id, queue: "test" }`
       - This endpoint serves as the verification mechanism that the full pipeline works:
         app enqueues → Redis → worker picks up → processes → emits notification

    3. Create `src/app/api/jobs/[jobId]/retry/route.ts` — Job retry endpoint:
       - POST handler: requires auth (any authenticated user can retry their own failed jobs)
       - Extract `jobId` from URL params
       - Look up the failed job across all queues in ALL_QUEUES:
         - For each queue: try `queue.getJob(jobId)` until found
         - If not found: return 404 `{ error: "Job nicht gefunden" }`
       - Validate the job is in "failed" state: `if (job.failedReason === undefined)` → 400 `{ error: "Job ist nicht fehlgeschlagen" }`
       - Security check: compare `job.data.userId` to `session.user.id` (unless user is ADMIN — admins can retry any job)
         - If mismatch: return 403 `{ error: "Keine Berechtigung" }`
       - Retry the job: `await job.retry()`
       - Return 200 `{ success: true, jobId: job.id, queue: job.queueName }`
       - Per locked decision: this endpoint is called by the "Erneut versuchen" button in notification-center.tsx
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer && npx tsc --noEmit --pretty 2>&1 | head -30</automated>
    <manual>After docker compose up: curl http://localhost:3000/api/health returns JSON with all service statuses</manual>
  </verify>
  <done>
    - GET /api/health returns JSON with status of PostgreSQL, Redis, MinIO, Meilisearch, OnlyOffice, and worker
    - Response includes latency per service, overall status (healthy/degraded), timestamp, and uptime
    - POST /api/admin/test-job enqueues a BullMQ job (ADMIN only) and returns jobId
    - POST /api/jobs/:jobId/retry retries a failed job (owner or ADMIN only) and returns success
    - Docker Compose health checks can use /api/health endpoint
  </done>
</task>

</tasks>

<verification>
1. `docker compose up -d` starts all services including redis and worker without errors
2. `curl http://localhost:3000/api/health` returns JSON with redis: healthy and postgres: healthy
3. Worker container logs show "Worker started" message in structured JSON format
4. Enqueuing a test job via POST /api/admin/test-job results in the worker picking it up within 5 seconds (visible in worker logs)
5. `docker compose stop worker` triggers graceful shutdown — worker logs show "Received shutdown signal" followed by "All workers closed"
6. `npx prisma db push` succeeds with new Notification and SystemSetting models
</verification>

<success_criteria>
- Redis 7 runs in Docker Compose with AOF persistence and noeviction policy
- Worker process runs as separate container, connects to Redis, processes BullMQ jobs
- Graceful shutdown completes in-flight jobs before exit on SIGTERM
- Health endpoint reports status of all infrastructure services
- Structured JSON logging works in both app and worker
- esbuild produces valid bundles for both server.ts and worker.ts
- All new TypeScript code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/01-infrastructure-foundation/01-01-SUMMARY.md`
</output>
