---
phase: 13-hybrid-search-reranking
plan: 03
type: execute
wave: 2
depends_on:
  - 13-01
  - 13-02
files_modified:
  - src/lib/queue/processors/embedding.processor.ts
  - src/app/api/ki-chat/route.ts
autonomous: true
requirements:
  - RAGQ-01
  - RAGQ-03

must_haves:
  truths:
    - "New documents embedded via BullMQ produce PARENT + CHILD chunks (not STANDALONE)"
    - "Helena's ki-chat responses include sources from both Meilisearch BM25 and pgvector — RRF fusion is active"
    - "LLM system prompt uses contextContent (parent chunk, ~2000 tokens) not content (child chunk, 500 tokens)"
    - "Confidence scoring uses RRF score from HybridSearchResult, not raw cosine score"
    - "Pre-Phase-13 STANDALONE chunks are still retrieved correctly — no regression for existing documents"
  artifacts:
    - path: "src/lib/queue/processors/embedding.processor.ts"
      provides: "Parent-child embedding pipeline via chunkDocumentParentChild + insertParentChildChunks"
      exports: ["processEmbeddingJob"]
    - path: "src/app/api/ki-chat/route.ts"
      provides: "hybridSearch replaces searchSimilar; contextContent used in LLM prompt"
      exports: ["POST"]
  key_links:
    - from: "src/lib/queue/processors/embedding.processor.ts"
      to: "src/lib/embedding/chunker.ts"
      via: "chunkDocumentParentChild() (replaces chunkDocument)"
      pattern: "chunkDocumentParentChild"
    - from: "src/lib/queue/processors/embedding.processor.ts"
      to: "src/lib/embedding/vector-store.ts"
      via: "insertParentChildChunks() (replaces insertChunks)"
      pattern: "insertParentChildChunks"
    - from: "src/app/api/ki-chat/route.ts"
      to: "src/lib/embedding/hybrid-search.ts"
      via: "hybridSearch() call replacing searchSimilar()"
      pattern: "hybridSearch"
    - from: "src/app/api/ki-chat/route.ts"
      to: "HybridSearchResult.contextContent"
      via: "system prompt builder uses contextContent (parent 2000 tokens) not content"
      pattern: "contextContent"
---

<objective>
Wire the Phase 13 pipeline into the two live execution paths: the BullMQ embedding processor (new documents) and the ki-chat route (retrieval + LLM). This is the final Wave 2 wiring plan — Plans 01 and 02 must be complete.

Purpose: Phase 13 is not active until these two files reference the new functions. The embedding processor switches new documents to parent-child chunking. The ki-chat route activates hybrid retrieval with RRF + reranking for all users.

Output: Both files are updated and the full hybrid search pipeline is live. Pre-Phase-13 STANDALONE documents continue working without re-embedding.
</objective>

<execution_context>
@/Users/patrickbaumfalk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrickbaumfalk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/13-hybrid-search-reranking/13-01-SUMMARY.md
@.planning/phases/13-hybrid-search-reranking/13-02-SUMMARY.md
@.planning/phases/13-hybrid-search-reranking/13-RESEARCH.md

<interfaces>
<!-- Contracts from Plan 01 and Plan 02 that this plan wires together. -->

From src/lib/embedding/hybrid-search.ts (Plan 01):
```typescript
export interface HybridSearchResult {
  id: string;
  dokumentId: string;
  dokumentName: string;
  akteAktenzeichen: string;
  akteBeschreibung: string;
  content: string;          // child/standalone chunk content — retrieval match evidence
  contextContent: string;   // parent content (2000 tokens) or STANDALONE content — for LLM prompt
  score: number;            // RRF score
  sources: ('bm25' | 'vector')[];
}

export async function hybridSearch(
  queryText: string,
  queryEmbedding: number[],
  opts: {
    akteId?: string;
    crossAkte?: boolean;
    userId?: string;
    bm25Limit?: number;
    vectorLimit?: number;
    finalLimit?: number;
  }
): Promise<HybridSearchResult[]>
```

From src/lib/embedding/chunker.ts (Plan 02):
```typescript
export async function chunkDocumentParentChild(text: string): Promise<{
  parent: { content: string; index: number };
  children: { content: string; index: number }[];
}[]>
```

From src/lib/embedding/vector-store.ts (Plan 02):
```typescript
export async function insertParentChildChunks(
  dokumentId: string,
  chunks: Array<{
    parent: { content: string; index: number };
    children: Array<{ content: string; index: number; embedding: number[] }>;
  }>,
  modelVersion: string
): Promise<void>
```

Current ki-chat route RAG retrieval section (to replace):
```typescript
// CURRENT (lines ~282-302):
sources = await searchSimilar(queryEmbedding, {
  akteId: akteId ?? undefined,
  crossAkte,
  userId,
  limit: 10,
});
// sources: SearchResult[] — uses src.content for LLM prompt

// CURRENT system prompt builder (line ~319):
systemPrompt += `\n[${i + 1}] Dokument: ${src.dokumentName} (Akte: ${src.akteAktenzeichen})\n${src.content}\n`;

// CURRENT sources metadata for client (line ~333-340):
const sourcesData = sources.map((src, i) => ({
  index: i + 1, dokumentId: src.dokumentId, name: src.dokumentName,
  akteAktenzeichen: src.akteAktenzeichen, passage: src.content.slice(0, 200), score: src.score,
}));
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Switch embedding processor to parent-child pipeline</name>
  <files>src/lib/queue/processors/embedding.processor.ts</files>
  <action>
Read current `src/lib/queue/processors/embedding.processor.ts` first.

Make the following targeted changes:

**Change 1: Update imports** — replace `chunkDocument` with `chunkDocumentParentChild`, replace `insertChunks` with `insertParentChildChunks`:
```typescript
// REMOVE:
import { chunkDocument } from "@/lib/embedding/chunker";
import { insertChunks } from "@/lib/embedding/vector-store";

// ADD:
import { chunkDocumentParentChild } from "@/lib/embedding/chunker";
import { insertParentChildChunks } from "@/lib/embedding/vector-store";
```

**Change 2: Replace chunking step** — swap `chunkDocument` call for `chunkDocumentParentChild`:
```typescript
// REMOVE:
const chunks = await chunkDocument(ocrText);
if (chunks.length === 0) { ... }

// ADD:
const parentChildGroups = await chunkDocumentParentChild(ocrText);
if (parentChildGroups.length === 0) {
  log.info({ dokumentId }, "[Embedding] No chunks produced (empty text), skipping");
  return;
}
```

**Change 3: Replace embedding generation loop** — embed only CHILD chunks (not parent chunks, which are stored unembedded):
```typescript
// REMOVE the entire existing batch loop (generates embeddedChunks array)

// ADD:
const embeddedGroups: Array<{
  parent: { content: string; index: number };
  children: Array<{ content: string; index: number; embedding: number[] }>;
}> = [];

let totalChildCount = 0;
for (const group of parentChildGroups) {
  totalChildCount += group.children.length;
}

log.info(
  { dokumentId, parentCount: parentChildGroups.length, childCount: totalChildCount },
  "[Embedding] Chunked document (parent-child), generating child embeddings..."
);

let processedChildren = 0;
for (const group of parentChildGroups) {
  const embeddedChildren: Array<{ content: string; index: number; embedding: number[] }> = [];

  const BATCH_SIZE = 5;
  for (let i = 0; i < group.children.length; i += BATCH_SIZE) {
    const batch = group.children.slice(i, i + BATCH_SIZE);
    for (const child of batch) {
      const embedding = await generateEmbedding(child.content);
      embeddedChildren.push({ content: child.content, index: child.index, embedding });
      processedChildren++;
    }

    // Update job progress for UI feedback
    const progress = Math.round((processedChildren / totalChildCount) * 100);
    await job.updateProgress(progress);
  }

  embeddedGroups.push({ parent: group.parent, children: embeddedChildren });
}
```

**Change 4: Replace storage call**:
```typescript
// REMOVE:
await insertChunks(dokumentId, embeddedChunks, MODEL_VERSION);

// ADD:
await insertParentChildChunks(dokumentId, embeddedGroups, MODEL_VERSION);
```

**Change 5: Update log message and AI scan trigger** — replace `embeddedChunks.length` with `totalChildCount`, replace `embeddedChunks.map(c => c.content).join(...)` with the child texts:
```typescript
log.info(
  { dokumentId, parentCount: parentChildGroups.length, childCount: totalChildCount, modelVersion: MODEL_VERSION },
  "[Embedding] Successfully embedded and stored parent-child chunks"
);

// For AI scan: concatenate all child texts (same total content as before)
const fullText = embeddedGroups.flatMap(g => g.children.map(c => c.content)).join("\n\n");
```

Preserve ALL other logic unchanged: Ollama availability check, job progress, AI scan trigger, error handling.
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer && npx tsc --noEmit --skipLibCheck 2>&1 | grep -E "embedding.processor|error TS" | head -10</automated>
  </verify>
  <done>embedding.processor.ts imports chunkDocumentParentChild + insertParentChildChunks, TypeScript compiles without errors, AI scan trigger and error handling unchanged</done>
</task>

<task type="auto">
  <name>Task 2: Wire ki-chat route to hybridSearch</name>
  <files>src/app/api/ki-chat/route.ts</files>
  <action>
Read current `src/app/api/ki-chat/route.ts` first.

Make the following targeted changes:

**Change 1: Update imports** — replace `searchSimilar` + `SearchResult` with `hybridSearch` + `HybridSearchResult`:
```typescript
// REMOVE:
import { searchSimilar, type SearchResult } from "@/lib/embedding/vector-store";

// ADD:
import { hybridSearch, type HybridSearchResult } from "@/lib/embedding/hybrid-search";
```

**Change 2: Update sources variable type**:
```typescript
// REMOVE:
let sources: SearchResult[] = [];

// ADD:
let sources: HybridSearchResult[] = [];
```

**Change 3: Replace searchSimilar call with hybridSearch**:
```typescript
// REMOVE:
sources = await searchSimilar(queryEmbedding, {
  akteId: akteId ?? undefined,
  crossAkte,
  userId,
  limit: 10,
});

// ADD:
sources = await hybridSearch(queryText, queryEmbedding, {
  akteId: akteId ?? undefined,
  crossAkte,
  userId,
  bm25Limit: 50,
  vectorLimit: 50,
  finalLimit: 10,
});
```

**Change 4: Update confidenceFlag logic** — use RRF score instead of raw cosine similarity. RRF scores are much smaller numbers (max ~1/60 ≈ 0.017 for rank 1), so adjust threshold:
```typescript
// REMOVE:
const maxScore = Math.max(...sources.map((s) => s.score));
confidenceFlag = maxScore < 0.3 ? "low" : "ok";

// ADD:
// RRF scores: rank-1 = 1/(60+1) ≈ 0.016. Threshold: any result = "ok"; no results = "none"
// (confidenceFlag "low" reserved for future quality scoring; RRF results are always meaningful)
confidenceFlag = sources.length > 0 ? "ok" : "none";
```

**Change 5: Update system prompt builder** — use `contextContent` (parent chunk, ~2000 tokens) instead of `content` (child chunk, ~500 tokens):
```typescript
// REMOVE:
systemPrompt += `\n[${i + 1}] Dokument: ${src.dokumentName} (Akte: ${src.akteAktenzeichen})\n${src.content}\n`;

// ADD:
systemPrompt += `\n[${i + 1}] Dokument: ${src.dokumentName} (Akte: ${src.akteAktenzeichen})\n${src.contextContent}\n`;
```

**Change 6: Update sourcesData metadata** — passage preview uses `content` (child chunk, 500 tokens) which is appropriate for the UI preview; contextContent stays in the system prompt only:
```typescript
// Keep src.content.slice(0, 200) for the passage preview — this is correct.
// Add sources field to show which retrieval sources contributed:
const sourcesData = sources.map((src, i) => ({
  index: i + 1,
  dokumentId: src.dokumentId,
  name: src.dokumentName,
  akteAktenzeichen: src.akteAktenzeichen,
  passage: src.content.slice(0, 200),
  score: src.score,
  sources: src.sources,  // NEW: ['bm25', 'vector'] | ['bm25'] | ['vector']
}));
```

Preserve ALL other logic unchanged: auth, aktenKontextBlock, token tracking, conversation save/update, streaming response.
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer && npx tsc --noEmit --skipLibCheck 2>&1 | grep -E "ki-chat|error TS" | head -20</automated>
  </verify>
  <done>ki-chat/route.ts imports hybridSearch, uses contextContent in system prompt, sourcesData has sources field, TypeScript compiles without errors</done>
</task>

</tasks>

<verification>
Run full TypeScript check across the project: `cd /Users/patrickbaumfalk/Projekte/AI-Lawyer && npx tsc --noEmit --skipLibCheck 2>&1 | head -40`

Expected: zero TypeScript errors.

Verify hybrid search wiring with grep:
```bash
grep -n "hybridSearch\|contextContent" /Users/patrickbaumfalk/Projekte/AI-Lawyer/src/app/api/ki-chat/route.ts
grep -n "chunkDocumentParentChild\|insertParentChildChunks" /Users/patrickbaumfalk/Projekte/AI-Lawyer/src/lib/queue/processors/embedding.processor.ts
```

Both files must show the new function names.
</verification>

<success_criteria>
- `embedding.processor.ts` uses `chunkDocumentParentChild` and `insertParentChildChunks`; no import of `chunkDocument` or `insertChunks`
- `ki-chat/route.ts` imports `hybridSearch` from `hybrid-search.ts`; LLM prompt uses `src.contextContent`; `sourcesData` includes `sources` field
- `npx tsc --noEmit --skipLibCheck` passes with zero errors across the project
- Phase 13 pipeline is end-to-end: new document → BullMQ → parent-child chunks in DB; Helena query → hybridSearch → BM25 + vector → RRF → reranker → parent content in system prompt
</success_criteria>

<output>
After completion, create `.planning/phases/13-hybrid-search-reranking/13-03-SUMMARY.md`
</output>
