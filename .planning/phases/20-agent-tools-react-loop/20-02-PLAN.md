---
phase: 20-agent-tools-react-loop
plan: 02
type: execute
wave: 2
depends_on: ["20-01"]
files_modified:
  - src/lib/helena/orchestrator.ts
  - src/lib/helena/token-budget.ts
  - src/lib/helena/stall-detector.ts
autonomous: true
requirements: [AGNT-01, AGNT-06]

must_haves:
  truths:
    - "runAgent() executes a ReAct loop up to maxSteps (5 inline, 20 background) then returns a fallback message"
    - "Token budget manager truncates oldest tool results when approaching 75% of context window"
    - "Stall detector identifies duplicate tool calls (same name+params 2x) and no-new-info (3 consecutive identical results)"
    - "When stall detected, agent is forced to give final answer via injected system message"
    - "onStepUpdate callback fires after every step with step number, tool name, and result summary"
    - "AbortSignal terminates the loop between steps"
    - "When LLM returns multiple tool_calls in one step, they execute concurrently via AI SDK or explicit Promise.all wrapping"
  artifacts:
    - path: "src/lib/helena/orchestrator.ts"
      provides: "Core ReAct agent loop wrapping AI SDK generateText"
      exports: ["runAgent", "AgentRunOptions", "AgentRunResult", "StepUpdate"]
    - path: "src/lib/helena/token-budget.ts"
      provides: "Token estimation and FIFO message truncation"
      exports: ["estimateTokens", "getContextWindow", "truncateMessages"]
    - path: "src/lib/helena/stall-detector.ts"
      provides: "Stall detection logic for agent loop"
      exports: ["StallDetector", "createStallDetector"]
  key_links:
    - from: "src/lib/helena/orchestrator.ts"
      to: "ai"
      via: "generateText({ tools, maxSteps, onStepFinish })"
      pattern: "generateText"
    - from: "src/lib/helena/orchestrator.ts"
      to: "src/lib/helena/token-budget.ts"
      via: "truncateMessages in onStepFinish"
      pattern: "truncateMessages"
    - from: "src/lib/helena/orchestrator.ts"
      to: "src/lib/helena/stall-detector.ts"
      via: "detectStall check in onStepFinish"
      pattern: "stallDetector\\.record|stallDetector\\.isStalled"
    - from: "src/lib/helena/orchestrator.ts"
      to: "src/lib/helena/audit-logger.ts"
      via: "logToolCall in onStepFinish"
      pattern: "logToolCall"
---

<objective>
Build the ReAct agent orchestrator that wraps AI SDK v4.3.19's `generateText({ maxSteps })` with stall detection, token budget management, step progress callbacks, audit logging, and abort support.

Purpose: This is the core reasoning engine that powers all Helena agent features. Phase 21 (Task-System), Phase 22 (Orchestrator), Phase 24 (Scanner), and Phase 25 (Memory) all call `runAgent()`.

Output: Three focused modules (orchestrator, token-budget, stall-detector) that compose into a robust bounded agent loop.
</objective>

<execution_context>
@/Users/patrickbaumfalk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrickbaumfalk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/20-agent-tools-react-loop/20-RESEARCH.md
@.planning/phases/20-agent-tools-react-loop/20-01-SUMMARY.md

<interfaces>
<!-- From Plan 01 outputs (types the orchestrator consumes) -->

From src/lib/helena/tools/types.ts (created in Plan 01):
```typescript
export interface ToolContext {
  prisma: PrismaClient;
  userId: string;
  userRole: UserRole;
  akteId: string | null;
  akteAccessFilter: Record<string, any>;
  helenaUserId: string;
  cache: ToolCache;
  abortSignal?: AbortSignal;
}

export interface ToolResult<T = unknown> {
  data?: T;
  error?: string;
  source?: SourceAttribution;
}
```

From src/lib/helena/tools/index.ts (created in Plan 01):
```typescript
export interface CreateHelenaToolsOptions {
  prisma: PrismaClient;
  userId: string;
  userRole: UserRole;
  akteId: string | null;
  helenaUserId: string;
}
export function createHelenaTools(options: CreateHelenaToolsOptions): Record<string, CoreTool>;
```

From src/lib/helena/audit-logger.ts (created in Plan 01):
```typescript
export function logToolCall(entry: { toolName: string; params: Record<string, unknown>; resultSummary: string; userId: string; akteId: string | null; durationMs: number }): void;
```

From src/lib/helena/system-prompt.ts (created in Plan 01):
```typescript
export function buildSystemPrompt(opts: { tools: Record<string, any>; akteId: string | null; userName: string; helenaMemory?: any }): string;
```

From AI SDK v4.3.19 (installed):
```typescript
import { generateText, type CoreMessage, type LanguageModel, type CoreTool } from "ai";
// generateText accepts: model, tools, system, messages, maxSteps, abortSignal, experimental_repairToolCall, onStepFinish
// onStepFinish receives: { text, toolCalls, toolResults, usage, finishReason, stepType, response }
```

From src/lib/ai/provider.ts:
```typescript
export async function getModel(): Promise<LanguageModel>;
export async function getModelName(): Promise<string>;
```

From src/lib/ai/token-tracker.ts:
```typescript
export async function wrapWithTracking<T>(result: T, metadata: { userId: string; akteId?: string | null; funktion: AiFunktion; provider: string; model: string }): Promise<T>;
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Token budget manager and stall detector</name>
  <files>
    src/lib/helena/token-budget.ts
    src/lib/helena/stall-detector.ts
  </files>
  <action>
**src/lib/helena/token-budget.ts** -- Token estimation and FIFO message truncation:

- `estimateTokens(text: string): number` -- Approximate: `Math.ceil(text.length / 3.5)` (conservative for mixed German/English). If input is not a string, JSON.stringify it first.

- `getContextWindow(modelName: string): number` -- Lookup table:
  - `qwen3.5:35b` = 32768
  - `gpt-4o` = 128000
  - `claude-sonnet-4-20250514` = 200000
  - Any model containing "lfm" = 32768
  - default = 32768
  Use `modelName.includes(key)` matching, case-insensitive.

- `estimateMessagesTokens(messages: CoreMessage[]): number` -- Iterate messages, sum `estimateTokens` on each message's content (handling string content, array content with text parts, tool-call content, tool-result content). Add ~4 tokens per message for role/metadata overhead.

- `truncateMessages(messages: CoreMessage[], contextWindow: number, budgetPercent?: number): CoreMessage[]` -- FIFO truncation:
  1. Calculate threshold: `contextWindow * (budgetPercent ?? 0.75)`
  2. If total estimated tokens < threshold, return messages unchanged
  3. Otherwise: Keep system messages, keep the first user message, keep the last 3 assistant+tool messages. Remove oldest tool-result messages first (they're the biggest), then oldest assistant messages.
  4. Return truncated array
  5. Never remove system or the original user message

Export all 4 functions.

**src/lib/helena/stall-detector.ts** -- Stall detection state machine:

- `createStallDetector(): StallDetector` -- Factory function returning a detector instance

- `StallDetector` interface:
  ```typescript
  interface StallDetector {
    record(step: { toolName: string; params: Record<string, unknown>; resultHash: string }): void;
    isStalled(): boolean;
    getForceMessage(): string;  // German message to inject as system message when stalled
    reset(): void;
  }
  ```

- Detection logic in `isStalled()`:
  1. **Duplicate call:** Same toolName + same JSON.stringify(sorted params) seen 2+ times -> stalled
  2. **No new info:** Last 3 consecutive steps have identical resultHash -> stalled
  3. Steps with no tool calls (pure text responses) do NOT count toward stall detection

- `getForceMessage()` returns: `"Du wiederholst dich. Gib jetzt deine beste Antwort mit dem, was du bisher weisst. Fasse zusammen und antworte dem Nutzer."`

- `record()` stores step info in an internal array. Use a simple hash for resultHash comparison: `createHash('md5').update(JSON.stringify(result)).digest('hex')` from Node crypto. Or simpler: just compare stringified results directly if under 10KB, MD5 if over.

- For the params comparison: sort object keys before stringifying to ensure deterministic comparison
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer/.claude/worktrees/dynamic-sleeping-dream && npx tsc --noEmit --pretty 2>&1 | head -50</automated>
  </verify>
  <done>
    - token-budget.ts exports estimateTokens, getContextWindow, estimateMessagesTokens, truncateMessages
    - stall-detector.ts exports createStallDetector with StallDetector interface
    - Stall detection catches duplicate (name+params 2x) and no-new-info (3 consecutive same result)
    - truncateMessages preserves system and first user message, removes oldest tool results first
    - All files compile without TypeScript errors
  </done>
</task>

<task type="auto">
  <name>Task 2: ReAct orchestrator with generateText wrapper</name>
  <files>
    src/lib/helena/orchestrator.ts
  </files>
  <action>
**src/lib/helena/orchestrator.ts** -- Core ReAct agent loop:

Define types:
```typescript
export interface AgentRunOptions {
  model: LanguageModel;
  modelName: string;         // For token budget context window lookup
  tools: Record<string, CoreTool>;
  systemPrompt: string;
  messages: CoreMessage[];
  mode: "inline" | "background";
  userId: string;
  akteId: string | null;
  onStepUpdate?: (step: StepUpdate) => void;
  abortSignal?: AbortSignal;
  repairToolCall?: Function;  // Optional experimental_repairToolCall hook (from Plan 03)
}

export interface StepUpdate {
  stepNumber: number;
  maxSteps: number;
  toolName: string | null;    // null for text-only steps
  toolParams?: Record<string, unknown>;
  resultSummary: string;      // First 200 chars of result
  tokenEstimate: number;      // Running total token estimate
}

export interface AgentRunResult {
  text: string;               // Final agent response text
  steps: AgentStep[];         // Complete trace for HelenaTask.steps
  totalTokens: { prompt: number; completion: number };
  finishReason: string;       // "stop" | "length" | "tool-calls" | "stall" | "abort" | "timeout"
  stalled: boolean;
  truncated: boolean;         // Whether token budget caused truncation
}

export interface AgentStep {
  type: "thought" | "toolCall" | "toolResult" | "error";
  content: string;
  toolName?: string;
  toolParams?: Record<string, unknown>;
  timestamp: string;          // ISO string
}
```

Implement `runAgent(options: AgentRunOptions): Promise<AgentRunResult>`:

1. Determine caps: `maxSteps = mode === "inline" ? 5 : 20`, `timeout = mode === "inline" ? 30_000 : 180_000`

2. Create AbortController combining external abortSignal with timeout:
   ```typescript
   const controller = new AbortController();
   const timeoutId = setTimeout(() => controller.abort("timeout"), timeout);
   if (options.abortSignal) {
     options.abortSignal.addEventListener("abort", () => controller.abort("user-cancel"));
   }
   ```

3. Create stall detector: `const stallDetector = createStallDetector();`

4. Track steps: `const agentSteps: AgentStep[] = [];` and running token counts

5. **Parallel tool calls:** AI SDK v4.3.19's `generateText` with `maxSteps` already executes multiple tool_calls returned in a single LLM response concurrently via internal `Promise.all`. This is the default behavior -- no additional wrapping needed. Verify by checking that `toolCalls` in `onStepFinish` can be an array with multiple entries. Per user decision "Parallel tool calls: If the LLM returns multiple tool_calls in one response, execute them concurrently" -- this is satisfied by the SDK's built-in behavior. If during implementation you discover the SDK does NOT execute them concurrently (check by logging timing), wrap the tool execute functions in `Promise.all()` explicitly.

6. Call `generateText`:
   ```typescript
   const result = await generateText({
     model: options.model,
     tools: options.tools,
     system: options.systemPrompt,
     messages: workingMessages,  // mutable copy for truncation
     maxSteps,
     abortSignal: controller.signal,
     experimental_repairToolCall: options.repairToolCall,
     onStepFinish: async ({ text, toolCalls, toolResults, usage, finishReason }) => {
       // a) Record step in agentSteps trace
       // b) Record in stall detector (if tool call)
       // c) Audit log each tool call via logToolCall
       // d) Token budget: estimate total, truncate if > 75% window
       // e) Fire onStepUpdate callback
       // f) Check stall: if stallDetector.isStalled(), we need to force stop
       //    -> Unfortunately generateText doesn't support mid-loop abort from onStepFinish
       //    -> Workaround: inject stall force message into workingMessages and set a flag
       //    -> The flag will be checked after generateText returns
     },
   });
   ```

7. **Stall handling:** If stall was detected during the loop:
   - The `onStepFinish` callback should set `stallDetected = true`
   - After `generateText` returns, if `stallDetected` is true, set finishReason to "stall"
   - The stall message was already injected into context, so the LLM's last response should be the forced summary

8. **Post-processing:**
   - Clear timeout
   - Extract final text from result
   - Aggregate token usage from all steps
   - Build AgentRunResult with steps trace, token totals, finishReason
   - If `maxSteps` was hit and finishReason indicates tool-calls still pending, set text to fallback: "Ich habe die maximale Anzahl an Schritten erreicht. Hier ist meine bisherige Zusammenfassung: ..." + whatever partial text is available

9. **Error handling:** Wrap the entire generateText call in try-catch:
   - AbortError (timeout/cancel) -> return partial result with finishReason "timeout" or "abort"
   - Other errors -> return error result with finishReason "error", include error message

10. **Token tracking integration:** After successful run, call `wrapWithTracking` from `src/lib/ai/token-tracker.ts` for the aggregated usage.

NOTE on `experimental_repairToolCall`: This will be created in Plan 03 (response-guard.ts). For now, accept it as an optional parameter in AgentRunOptions. Plan 03 will pass `ollamaResponseGuard` when calling via `runHelenaAgent()`.
  </action>
  <verify>
    <automated>cd /Users/patrickbaumfalk/Projekte/AI-Lawyer/.claude/worktrees/dynamic-sleeping-dream && npx tsc --noEmit --pretty 2>&1 | head -50</automated>
  </verify>
  <done>
    - orchestrator.ts exports runAgent, AgentRunOptions, AgentRunResult, StepUpdate, AgentStep
    - runAgent wraps generateText with maxSteps (5/20), timeout (30s/3min), abort signal
    - onStepFinish performs: step recording, stall detection, audit logging, token budget truncation, progress callback
    - Stall detection injects force-answer message and sets finishReason "stall"
    - Token budget truncation removes oldest tool results when > 75% context window
    - AgentStep trace captures full reasoning chain for HelenaTask.steps JSON
    - Error handling returns partial results on timeout/cancel/error
    - Parallel tool calls are handled by AI SDK natively (or explicit Promise.all if SDK doesn't)
    - TypeScript compiles without errors
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. orchestrator.ts imports and uses token-budget.ts and stall-detector.ts
3. AgentRunResult.steps array matches HelenaTask.steps JSON format from Phase 19 schema
4. Token budget threshold is 75% (not 80% or 100%)
5. Stall detection uses sorted params for deterministic comparison
6. Parallel tool calls are addressed (SDK native or explicit wrapping)
</verification>

<success_criteria>
- runAgent({ model, tools, messages, mode:"inline" }) limits to 5 steps with 30s timeout
- runAgent({ model, tools, messages, mode:"background" }) limits to 20 steps with 3min timeout
- Token budget truncates oldest tool results first, preserves system + user messages
- Stall detector triggers on duplicate calls and no-new-info patterns
- Complete agent trace is captured in AgentStep[] format
- Multiple tool calls in one step execute concurrently
- All modules compile and can be imported
</success_criteria>

<output>
After completion, create `.planning/phases/20-agent-tools-react-loop/20-02-SUMMARY.md`
</output>
