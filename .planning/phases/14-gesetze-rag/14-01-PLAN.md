---
phase: 14-gesetze-rag
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/gesetze/github-client.ts
  - src/lib/gesetze/markdown-parser.ts
  - src/lib/gesetze/ingestion.ts
autonomous: true
requirements: [GESETZ-01, GESETZ-02]

must_haves:
  truths:
    - "Calling fetchAllGesetzeFiles() returns a list of {path, sha} objects for all index.md files in bundestag/gesetze"
    - "Calling parseGesetzeMarkdown(rawMarkdown, slug) returns LawParagraph[] with gesetzKuerzel, paragraphNr, titel, inhalt, stand, slug populated"
    - "encodingSmokePassed() returns false when content contains 'Â§', true otherwise"
    - "Calling upsertLawChunks(paragraphs, modelVersion) deletes existing law_chunks rows for that (gesetzKuerzel, paragraphNr) pair and inserts new ones with embedding + sourceUrl"
    - "Calling searchLawChunks(queryEmbedding, {limit:5}) returns top-5 LawChunkResult[] sorted by cosine similarity"
  artifacts:
    - path: "src/lib/gesetze/github-client.ts"
      provides: "GitHub API access — fetchAllGesetzeFiles(), fetchRawFileContent()"
      exports: ["fetchAllGesetzeFiles", "fetchRawFileContent"]
    - path: "src/lib/gesetze/markdown-parser.ts"
      provides: "bundestag/gesetze Markdown parser — LawParagraph type, parseGesetzeMarkdown(), encodingSmokePassed()"
      exports: ["LawParagraph", "parseGesetzeMarkdown", "encodingSmokePassed"]
    - path: "src/lib/gesetze/ingestion.ts"
      provides: "law_chunks DB operations — buildSourceUrl(), upsertLawChunks(), searchLawChunks(), LawChunkResult"
      exports: ["buildSourceUrl", "upsertLawChunks", "searchLawChunks", "LawChunkResult"]
  key_links:
    - from: "src/lib/gesetze/ingestion.ts"
      to: "law_chunks (postgres table)"
      via: "prisma.$executeRaw with pgvector.toSql(embedding)"
      pattern: "prisma\\.\\$executeRaw"
    - from: "src/lib/gesetze/ingestion.ts"
      to: "src/lib/embedding/embedder.ts"
      via: "generateEmbedding(text) for passage embedding"
      pattern: "generateEmbedding"
---

<objective>
Build the three core library files for Gesetze-RAG: GitHub API client, Markdown parser, and law_chunks DB ingestion/search layer.

Purpose: Plans 02 and 03 both depend on this library layer — the cron processor imports parseGesetzeMarkdown + upsertLawChunks, and ki-chat imports searchLawChunks. Building these first enables parallel execution of Plans 02 and 03.

Output:
- src/lib/gesetze/github-client.ts — fetchAllGesetzeFiles() + fetchRawFileContent()
- src/lib/gesetze/markdown-parser.ts — parseGesetzeMarkdown() + encodingSmokePassed() + LawParagraph type
- src/lib/gesetze/ingestion.ts — upsertLawChunks() + searchLawChunks() + buildSourceUrl() + LawChunkResult type
</objective>

<execution_context>
@/Users/patrickbaumfalk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrickbaumfalk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-gesetze-rag/14-RESEARCH.md

<interfaces>
<!-- Key contracts the executor needs. Verified from codebase. -->

From src/lib/embedding/embedder.ts:
```typescript
export const MODEL_VERSION = `${EMBEDDING_MODEL}@1.0`;  // e.g. "blaifa/multilingual-e5-large-instruct@1.0"
export async function generateEmbedding(text: string): Promise<number[]>;
// Uses "passage: " E5 prefix internally. Pass raw paragraph text — no prefix needed from caller.
```

From prisma/schema.prisma (Phase 12 LawChunk model — no migration needed):
```
model LawChunk {
  id            String    @id @default(cuid())
  gesetzKuerzel String    // "BGB"
  paragraphNr   String    // "§ 626"
  titel         String
  content       String    @db.Text   // child chunk for embedding
  parentContent String?   @db.Text   // same as content for Gesetze (§ = atomic unit)
  embedding     Unsupported("vector(1024)")?
  modelVersion  String
  syncedAt      DateTime  @default(now())
  sourceUrl     String?
  @@map("law_chunks")
}
```

pgvector pattern (from src/lib/embedding/vector-store.ts):
```typescript
import pgvector from "pgvector";
const vectorSql = pgvector.toSql(embedding);  // serialize number[] to pgvector wire format
await prisma.$executeRaw`INSERT INTO ... embedding = ${vectorSql}::vector`;
```

GitHub REST API (git trees recursive — verified):
- Endpoint: `GET https://api.github.com/repos/bundestag/gesetze/git/trees/{sha}?recursive=1`
- Get HEAD SHA: `GET https://api.github.com/repos/bundestag/gesetze/branches/master`
- Raw file: `https://raw.githubusercontent.com/bundestag/gesetze/master/{path}`
- Auth: optional `Authorization: Bearer ${GITHUB_TOKEN}` — 60 req/hr without, 5000/hr with
- Filter: only items where `type === "blob"` AND `path.endsWith("/index.md")` AND `path.split("/").length === 3`

bundestag/gesetze Markdown format (verified from direct file inspection):
```
---
Title: Bürgerliches Gesetzbuch
jurabk: BGB
slug: bgb
---
# Bürgerliches Gesetzbuch (BGB)
Zuletzt geändert durch
:   Art. 4 Abs. 6 G v. 7.5.2021 I 850
##### § 1 Beginn der Rechtsfähigkeit
Die Rechtsfähigkeit des Menschen beginnt mit der Vollendung der Geburt.
##### § 2 Eintritt der Volljährigkeit
...
```
- `jurabk` field = gesetzKuerzel (e.g. "BGB")
- `Zuletzt geändert durch\n:\s+(.+)` = stand string (store as-is; use syncedAt for display)
- `##### § N Title` (exactly 5 hashes) = paragraph heading
- Content between consecutive ##### headings = paragraph body
- Skip lines starting with `#` inside paragraph body (structural headings)

gesetze-im-internet.de canonical URLs (verified):
- Pattern: `https://www.gesetze-im-internet.de/{slug}/__{paragraphNumber}.html`
- e.g. BGB § 626 → `https://www.gesetze-im-internet.de/bgb/__626.html`
- Extract number from "§ 626" → "626" via regex `/§\s*(\w+)/`
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: GitHub API client + Markdown parser</name>
  <files>
    src/lib/gesetze/github-client.ts
    src/lib/gesetze/markdown-parser.ts
  </files>
  <action>
Create `src/lib/gesetze/github-client.ts`:

```typescript
/**
 * GitHub API client for bundestag/gesetze repository.
 * Fetches all Gesetz file paths+SHAs and raw Markdown content.
 * No external dependencies — uses native fetch (Node 18+).
 */

const GITHUB_API = "https://api.github.com";
const REPO_OWNER = "bundestag";
const REPO_REPO = "gesetze";
const RAW_BASE = `https://raw.githubusercontent.com/${REPO_OWNER}/${REPO_REPO}/master`;

export interface GitTreeItem {
  path: string;   // e.g. "b/bgb/index.md"
  type: string;   // "blob" | "tree"
  sha: string;    // blob SHA for change detection
}

function getGitHubHeaders(): Record<string, string> {
  const headers: Record<string, string> = {
    "Accept": "application/vnd.github+json",
    "X-GitHub-Api-Version": "2022-11-28",
  };
  if (process.env.GITHUB_TOKEN) {
    headers["Authorization"] = `Bearer ${process.env.GITHUB_TOKEN}`;
  } else {
    // Log warning once on first call — unauthenticated = 60 req/hr limit
    console.warn("[gesetze] GITHUB_TOKEN not set — unauthenticated rate limit (60 req/hr). Set GITHUB_TOKEN for 5000 req/hr.");
  }
  return headers;
}

/**
 * Fetch all index.md file paths and SHAs from bundestag/gesetze.
 * Uses git trees recursive API — one request for all ~2000 files.
 */
export async function fetchAllGesetzeFiles(): Promise<GitTreeItem[]> {
  const headers = getGitHubHeaders();

  // Step 1: Get HEAD commit SHA from master branch
  const branchRes = await fetch(
    `${GITHUB_API}/repos/${REPO_OWNER}/${REPO_REPO}/branches/master`,
    { headers, signal: AbortSignal.timeout(15_000) }
  );
  if (!branchRes.ok) {
    throw new Error(`GitHub branch fetch failed: ${branchRes.status} ${await branchRes.text().catch(() => "")}`);
  }
  const branch = await branchRes.json() as { commit: { sha: string } };
  const treeSha = branch.commit.sha;

  // Step 2: Fetch all files recursively (bundestag/gesetze ~2000 files, well under 100k limit)
  const treeRes = await fetch(
    `${GITHUB_API}/repos/${REPO_OWNER}/${REPO_REPO}/git/trees/${treeSha}?recursive=1`,
    { headers, signal: AbortSignal.timeout(30_000) }
  );
  if (!treeRes.ok) {
    throw new Error(`GitHub tree fetch failed: ${treeRes.status}`);
  }
  const tree = await treeRes.json() as { tree: GitTreeItem[]; truncated: boolean };

  if (tree.truncated) {
    // Should not happen for bundestag/gesetze — it has ~2000 files, limit is 100k nodes
    throw new Error("GitHub git tree response was truncated — unexpected for bundestag/gesetze");
  }

  // Filter to only {letter}/{slug}/index.md files (3-part path, ends in /index.md)
  return tree.tree.filter(item =>
    item.type === "blob" &&
    item.path.endsWith("/index.md") &&
    item.path.split("/").length === 3
  );
}

/**
 * Fetch raw Markdown content for a single Gesetz file.
 * @param path - File path as returned by fetchAllGesetzeFiles, e.g. "b/bgb/index.md"
 */
export async function fetchRawFileContent(path: string): Promise<string> {
  const url = `${RAW_BASE}/${path}`;
  const res = await fetch(url, { signal: AbortSignal.timeout(15_000) });
  if (!res.ok) {
    throw new Error(`Raw file fetch failed (${res.status}): ${url}`);
  }
  return res.text();
}
```

Create `src/lib/gesetze/markdown-parser.ts`:

```typescript
/**
 * Parser for bundestag/gesetze Markdown format.
 * Splits each Gesetz file into LawParagraph objects — one per § heading.
 * No external Markdown library needed — format uses a simple state machine.
 */

export interface LawParagraph {
  gesetzKuerzel: string; // from jurabk frontmatter, e.g. "BGB"
  paragraphNr: string;   // e.g. "§ 626"
  titel: string;         // e.g. "Fristlose Kündigung aus wichtigem Grund"
  inhalt: string;        // full paragraph body text (all Absätze joined)
  stand: string;         // raw "Zuletzt geändert durch" string — store as-is, use syncedAt for display
  slug: string;          // directory slug, e.g. "bgb"
}

/**
 * Smoke test for encoding correctness.
 * Returns false (skip file) if § character appears as "Â§" — classic UTF-8 bytes
 * decoded as latin-1 (mojibake). Log a warning; do NOT throw. Caller must skip the file.
 */
export function encodingSmokePassed(content: string, slug: string): boolean {
  if (content.includes("Â§")) {
    console.warn(`[gesetze-sync] ENCODING FAIL: §-Zeichen appears as "Â§" in ${slug} — skipping file`);
    return false;
  }
  return true;
}

/**
 * Parse a bundestag/gesetze Markdown file into structured LawParagraph objects.
 * One object per ##### § heading. Paragraphs with < 10 chars body are skipped.
 *
 * @param markdown - Raw file content from fetchRawFileContent()
 * @param fallbackSlug - Directory slug (e.g. "bgb") used if jurabk not found in frontmatter
 */
export function parseGesetzeMarkdown(markdown: string, fallbackSlug: string): LawParagraph[] {
  const results: LawParagraph[] = [];

  // 1. Extract YAML frontmatter (between first two "---" delimiters)
  const frontmatterMatch = markdown.match(/^---\n([\s\S]*?)\n---/);
  const frontmatter = frontmatterMatch?.[1] ?? "";

  // Extract gesetzKuerzel from jurabk field
  const jurabkMatch = frontmatter.match(/^jurabk:\s*(.+)$/m);
  const gesetzKuerzel = jurabkMatch?.[1]?.trim() ?? fallbackSlug.toUpperCase();

  // Extract amendment date as raw string (free-text German legal citation format)
  const standMatch = markdown.match(/Zuletzt ge[äa]ndert durch\n:\s+(.+)/);
  const stand = standMatch?.[1]?.trim() ?? "Stand unbekannt";

  // 2. Line-by-line state machine — split by ##### headings
  const lines = markdown.split("\n");
  let currentParagraphNr: string | null = null;
  let currentTitel = "";
  let contentLines: string[] = [];

  const saveCurrent = () => {
    if (currentParagraphNr === null) return;
    const inhalt = contentLines.join("\n").trim();
    if (inhalt.length >= 10) {
      results.push({
        gesetzKuerzel,
        paragraphNr: currentParagraphNr,
        titel: currentTitel,
        inhalt,
        stand,
        slug: fallbackSlug,
      });
    }
  };

  for (const line of lines) {
    // Match exactly 5 hashes: ##### § N Title
    const paraMatch = line.match(/^##### (§\s*\S+)\s+(.+)$/);
    if (paraMatch) {
      saveCurrent();
      currentParagraphNr = paraMatch[1].trim();  // "§ 626"
      currentTitel = paraMatch[2].trim();          // "Fristlose Kündigung..."
      contentLines = [];
    } else if (currentParagraphNr !== null) {
      // Inside a paragraph — skip sub-section headings (##, ###, ####) which are
      // organizational structure, not paragraph content
      if (!line.startsWith("#")) {
        contentLines.push(line);
      }
    }
  }
  saveCurrent(); // Save last paragraph

  return results;
}
```
  </action>
  <verify>npx tsc --noEmit 2>&1 | grep -E "gesetze/(github-client|markdown-parser)" | head -20; echo "TypeScript check done"</verify>
  <done>Both files compile without TypeScript errors. fetchAllGesetzeFiles, fetchRawFileContent, parseGesetzeMarkdown, encodingSmokePassed, and LawParagraph are all exported. State machine correctly identifies ##### § headings and skips ## through #### structural headings.</done>
</task>

<task type="auto">
  <name>Task 2: law_chunks ingestion + search library</name>
  <files>
    src/lib/gesetze/ingestion.ts
  </files>
  <action>
Create `src/lib/gesetze/ingestion.ts`:

```typescript
/**
 * law_chunks DB ingestion and search for Gesetze-RAG.
 *
 * upsertLawChunks(): Delete+re-insert law_chunks rows for changed Gesetze.
 * searchLawChunks(): pgvector cosine similarity search for ki-chat Chain D.
 * buildSourceUrl():  Construct canonical gesetze-im-internet.de URL.
 */

import pgvector from "pgvector";
import { prisma } from "@/lib/db";
import { generateEmbedding, MODEL_VERSION } from "@/lib/embedding/embedder";
import type { LawParagraph } from "@/lib/gesetze/markdown-parser";

// ---------------------------------------------------------------------------
// Types
// ---------------------------------------------------------------------------

export interface LawChunkResult {
  id: string;
  gesetzKuerzel: string;
  paragraphNr: string;
  titel: string;
  content: string;
  syncedAt: Date;
  sourceUrl: string | null;
  score: number;
}

// ---------------------------------------------------------------------------
// URL builder
// ---------------------------------------------------------------------------

/**
 * Build canonical gesetze-im-internet.de URL for a paragraph.
 * Pattern: https://www.gesetze-im-internet.de/{slug}/__{nr}.html
 * Verified: https://www.gesetze-im-internet.de/bgb/__626.html returns 200.
 */
export function buildSourceUrl(slug: string, paragraphNr: string): string {
  const numMatch = paragraphNr.match(/§\s*(\w+)/);
  if (!numMatch) return `https://www.gesetze-im-internet.de/${slug}/`;
  return `https://www.gesetze-im-internet.de/${slug}/__${numMatch[1]}.html`;
}

// ---------------------------------------------------------------------------
// Ingestion
// ---------------------------------------------------------------------------

/**
 * Upsert law_chunks for a list of paragraphs from a single Gesetz.
 * Strategy: DELETE existing rows for each (gesetzKuerzel, paragraphNr) pair,
 * then INSERT with fresh embedding. Idempotent — safe to re-run on changed files.
 *
 * parentContent = content (§ paragraphs are atomic units; no further sub-chunking needed
 * for paragraphs < 4000 chars; for longer ones, truncate parentContent at 8000 chars).
 *
 * @returns { inserted, skipped } counts
 */
export async function upsertLawChunks(
  paragraphs: LawParagraph[],
  modelVersion: string = MODEL_VERSION
): Promise<{ inserted: number; skipped: number }> {
  let inserted = 0;
  let skipped = 0;

  for (const para of paragraphs) {
    if (!para.inhalt || para.inhalt.length < 10) {
      skipped++;
      continue;
    }

    try {
      const childContent = para.inhalt;
      // parentContent = same as content for Gesetze — § is the atomic unit.
      // Cap at 8000 chars if a rare long paragraph exceeds practical limit.
      const parentContent = para.inhalt.length > 8000
        ? para.inhalt.slice(0, 8000)
        : para.inhalt;

      const embedding = await generateEmbedding(childContent);
      const sourceUrl = buildSourceUrl(para.slug, para.paragraphNr);
      const vectorSql = pgvector.toSql(embedding);

      // Delete existing chunk for this law+paragraph (idempotent re-index)
      await prisma.$executeRaw`
        DELETE FROM law_chunks
        WHERE "gesetzKuerzel" = ${para.gesetzKuerzel}
          AND "paragraphNr" = ${para.paragraphNr}
      `;

      // Insert with embedding
      await prisma.$executeRaw`
        INSERT INTO law_chunks (
          id, "gesetzKuerzel", "paragraphNr", titel, content, "parentContent",
          embedding, "modelVersion", "syncedAt", "sourceUrl"
        )
        VALUES (
          gen_random_uuid(),
          ${para.gesetzKuerzel},
          ${para.paragraphNr},
          ${para.titel},
          ${childContent},
          ${parentContent},
          ${vectorSql}::vector,
          ${modelVersion},
          NOW(),
          ${sourceUrl}
        )
      `;

      inserted++;
    } catch (err) {
      console.error(`[gesetze-ingestion] Failed to upsert ${para.gesetzKuerzel} ${para.paragraphNr}:`, err);
      skipped++;
    }
  }

  return { inserted, skipped };
}

// ---------------------------------------------------------------------------
// Search (used by ki-chat Chain D)
// ---------------------------------------------------------------------------

/**
 * Search law_chunks by vector cosine similarity.
 * Returns top-N results sorted by descending cosine similarity (score = 1 - distance).
 * Only returns results where embedding IS NOT NULL (paranoia guard for first-run edge cases).
 *
 * Called from ki-chat/route.ts Chain D — reuses queryEmbedding from Chain B.
 */
export async function searchLawChunks(
  queryEmbedding: number[],
  opts: { limit?: number; minScore?: number } = {}
): Promise<LawChunkResult[]> {
  const { limit = 5, minScore = 0.0 } = opts;
  const vectorSql = pgvector.toSql(queryEmbedding);

  type RawRow = {
    id: string;
    gesetzKuerzel: string;
    paragraphNr: string;
    titel: string;
    content: string;
    syncedAt: Date;
    sourceUrl: string | null;
    score: number;
  };

  const rows = await prisma.$queryRaw<RawRow[]>`
    SELECT
      id,
      "gesetzKuerzel",
      "paragraphNr",
      titel,
      content,
      "syncedAt",
      "sourceUrl",
      1 - (embedding <=> ${vectorSql}::vector) AS score
    FROM law_chunks
    WHERE embedding IS NOT NULL
    ORDER BY embedding <=> ${vectorSql}::vector ASC
    LIMIT ${limit}
  `;

  return rows
    .map(r => ({ ...r, score: Number(r.score) }))
    .filter(r => r.score >= minScore);
}
```

IMPORTANT: `setSettingTyped` does NOT exist in the codebase — only `updateSetting(key: string, value: string)` and `getSettingTyped<T>(key, default)`. The gesetze-sync processor (Plan 02) must use:
- Read: `getSettingTyped<Record<string,string>>("gesetze.sha_cache", {})` — but the Settings type will be "string" (not "json") unless explicitly created. Use `getSetting("gesetze.sha_cache")` and manual `JSON.parse`, `JSON.stringify` + `updateSetting("gesetze.sha_cache", JSON.stringify(cache))`.

Export the SHA cache helpers from ingestion.ts for use in the processor:

```typescript
// ---------------------------------------------------------------------------
// SHA cache (stored in SystemSetting as JSON string)
// ---------------------------------------------------------------------------

import { getSetting, updateSetting } from "@/lib/settings/service";

const SHA_CACHE_KEY = "gesetze.sha_cache";

/** Load SHA cache from Settings table. Returns {} if not yet set. */
export async function loadShaCache(): Promise<Record<string, string>> {
  try {
    const raw = await getSetting(SHA_CACHE_KEY);
    if (!raw) return {};
    return JSON.parse(raw) as Record<string, string>;
  } catch {
    return {};
  }
}

/** Persist SHA cache to Settings table as JSON string. */
export async function saveShaCache(cache: Record<string, string>): Promise<void> {
  await updateSetting(SHA_CACHE_KEY, JSON.stringify(cache));
}
```
  </action>
  <verify>npx tsc --noEmit 2>&1 | grep -E "gesetze/ingestion" | head -20; echo "TypeScript check done"</verify>
  <done>ingestion.ts compiles without errors. Exports: LawChunkResult, buildSourceUrl, upsertLawChunks, searchLawChunks, loadShaCache, saveShaCache. pgvector.toSql() used for vector serialization (same pattern as vector-store.ts). getSetting/updateSetting used for SHA cache (no setSettingTyped which doesn't exist).</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit 2>&1 | grep gesetze` — zero TypeScript errors in new files
- All three files exist: `ls src/lib/gesetze/`
- All exports present: `grep -n "^export" src/lib/gesetze/*.ts`
</verification>

<success_criteria>
Three library files exist under src/lib/gesetze/ with zero TypeScript errors. fetchAllGesetzeFiles, fetchRawFileContent, parseGesetzeMarkdown, encodingSmokePassed, upsertLawChunks, searchLawChunks, loadShaCache, saveShaCache are all exported. Plan 02 (processor) and Plan 03 (ki-chat) can import from these files without modification.
</success_criteria>

<output>
After completion, create `.planning/phases/14-gesetze-rag/14-01-SUMMARY.md` following the summary template.
</output>
